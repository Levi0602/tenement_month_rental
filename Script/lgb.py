import numpy as npimport pandas as pdimport timeimport datetimefrom scipy import sparsefrom sklearn.feature_extraction.text import CountVectorizerimport gcfrom sklearn.model_selection import KFold, cross_val_score, train_test_splitfrom sklearn.cross_validation import StratifiedKFoldfrom sklearn import preprocessingfrom sklearn.metrics import roc_auc_score, log_lossimport lightgbm as lgbimport datetimestart = time.time()print(datetime.datetime.now())def get_cv(data):    num_feature = data.drop(columns=['房屋朝向']).columns    data['new_con'] = data['小区名'].astype(str)    # , '地铁线路', '地铁站点', '区', '位置'    for i in ['房屋朝向', '位置', '装修情况', '时间', '楼层']:        data['new_con'] = data['new_con'].astype(str)+'_'+data[i].astype(str)    data['new_con'] = data['new_con'].apply(lambda x: ' '.join(x.split('_')))    # print(len(data))    total_feature = sparse.csr_matrix((len(data), 0))    cv = CountVectorizer(min_df=22)    # print(data['new_con'])    for feature in ['new_con']:        data[feature] = data[feature].astype(str)        # print(data[feature])        cv.fit(data[feature])        total_feature = sparse.hstack((total_feature, cv.transform(data[feature].astype(str))), 'csr', 'bool')    print('CountVectorizer_over!')    total_feature = sparse.hstack((sparse.csr_matrix(data[num_feature].astype('float32')), total_feature),                                  'csr').astype('float32')    print(total_feature)    return total_featuredef get_feature(data):    # data['卫的数量'] = (data['卫的数量'] * 10) ** 2    # data['卧室数量'] = (data['卧室数量'] * 10) ** 2    data['卧室_厅'] = data['卧室数量'] + data['厅的数量']    data['卧室_卫'] = data['卧室数量'] + data['卫的数量']    data['卫_厅'] = data['卫的数量'] + data['厅的数量']    data['卧室_厅_卫'] = data['卧室数量'] + data['厅的数量'] + data['卫的数量']    data['average_面积'] = (data['房屋面积'] / 0.000165) / data['卧室_厅_卫']    data['面积/卧室'] = (data['房屋面积'] / 0.000165) / (data['卧室数量'] + 1)    data['面积/厅'] = (data['房屋面积'] / 0.000165) / (data['厅的数量'] + 1)    data['面积/卫'] = (data['房屋面积'] / 0.000165) / (data['卫的数量'] + 1)    # data['面积/卧室_卫'] = (data['房屋面积'] / 0.000165) / (data['卧室数量'] + data['卫的数量'])    # data['卫*厅'] = data['卫的数量'] * data['厅的数量']    data['卫*卧室'] = data['卫的数量'] * data['卧室数量']    data['总楼层'] = data['总楼层'] / 0.018182    data['小区房屋出租数量'] = data['小区房屋出租数量'] / 0.003906    data['房屋面积'] = (data['房屋面积'] / 0.000165)    data['总楼层房屋面积'] = data['房屋面积'] * data['总楼层']    # data['距离'] = data['距离'] / 0.000833    data['距离'] = data['距离'].fillna(-1)    data['距离'] = (data['距离'] / 0.000833).astype(int)    # data['距离'] = (data['距离'] / 50).astype(int)    data['楼层/总楼层'] = data['楼层'] / data['总楼层']    data['楼层-0.5总楼层'] = abs(data['楼层'] - data['总楼层'] * 0.5)    # data['卧室-卫'] = data['卧室数量'] - data['卫的数量']    # data['卧室-厅'] = data['卧室数量'] - data['厅的数量']    # data['小区房屋总面积'] = data.groupby(['小区房屋出租数量']).apply(lambda x: x['房屋面积'].sum())    # print(data['小区房屋总面积'].mean())    print(data.shape)    return data# 加特征后用这个train = pd.read_csv('../data/train.csv')# train = train[:100]test = pd.read_csv('../data/test.csv')# test = test[:100]train = get_feature(train)test = get_feature(test)# 删除没用的特征drop = []train.drop(drop, axis=1, inplace=True)test.drop(drop, axis=1, inplace=True)y_train = train.pop('月租金')res = pd.DataFrame()res['id'] = test.pop('id').astype(int)# train['总楼层'] = train['总楼层']*100# test['总楼层'] = test['总楼层']*100data = pd.concat([train, test])data = data.fillna(-1)data = get_cv(data)train = data[:train.shape[0]]print(train.shape)test = data[train.shape[0]:]print(test.shape)# y_train = train.pop('月租金')# all_data = pd.concat([train, test])# res = pd.DataFrame()# res['id'] = test['id'].astype(int)X, y, X_test = train, y_train, testparams = {'boosting_type': 'gbdt',          'num_leaves': 68,          'max_depth': -1,          'objective': 'regression',          'learning_rate': 0.05,          'seed': 2018,          'num_threads': -1,          'max_bin': 425,          "metric": "rmse",          "bagging_fraction": 0.8,          'colsample_bytree': 0.9,          'subsample': 0.8,          # "lambda_l1": 0.1,          "lambda_l2": 0.2,          }# # 当前参数线下：1.24121# price mean:%d 8.033074515681161x_train = Xx_test = testx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.18, random_state=42)train_data = lgb.Dataset(x_train, label=y_train.values.flatten())val_data = lgb.Dataset(x_val, label=y_val.values.flatten())clf = lgb.train(params, train_set=train_data, num_boost_round=50000, valid_sets=[train_data, val_data],                  valid_names=['train', 'valid'], early_stopping_rounds=100, feval=None, verbose_eval=100)# importance = pd.DataFrame({#     'column': train.columns,#     'importance': clf.feature_importance(),# }).sort_values(by='importance')# pd.DataFrame(importance).to_csv('../data/feature_importance.csv', index=None)res['price'] = clf.predict(x_test)print('price mean:%d', res['price'].mean())# n = input('please input n:%d')# n = y.mean() / res['price'].mean()# res['price'] = res['price'] * int(n)now = datetime.datetime.now()now = now.strftime('%m-%d-%H-%M')res.to_csv("../result/baseline1.csv", index=None)end = time.time()print(datetime.datetime.now())print('all_time:', end-start)