import numpy as npimport pandas as pdimport timeimport datetimefrom scipy import sparsefrom sklearn.feature_extraction.text import CountVectorizerimport gcfrom sklearn.model_selection import KFold, cross_val_score, train_test_splitfrom sklearn.cross_validation import StratifiedKFoldfrom sklearn import preprocessingfrom sklearn.metrics import roc_auc_score, log_lossimport lightgbm as lgbimport datetimeimport copystart = time.time()print(datetime.datetime.now())def getDirection(df):    '''    将朝向拆开    '''    data = copy.deepcopy(df)    for i, j in zip(['东南', '东', '西北', '西南', '北', '南', '西', '东北'],     ["east_south", "east", "west_north", "west_south", "north", "south", "west", "east_north"]):        print(i)        data["is_"+j] = data["房屋朝向"].map(lambda x: 1 if i in x else 0)    columns = list(filter(lambda x: "is_" in x, data.columns))    return data[columns]def get_cv(data):    num_feature = data.drop(columns=['房屋朝向']).columns    data['new_con'] = data['小区名'].astype(str)    # , '地铁线路', '地铁站点', '区', '位置'    for i in ['房屋朝向', '位置', '装修情况', '时间', '楼层']:        data['new_con'] = data['new_con'].astype(str)+'_'+data[i].astype(str)    data['new_con'] = data['new_con'].apply(lambda x: ' '.join(x.split('_')))    # print(len(data))    total_feature = sparse.csr_matrix((len(data), 0))    cv = CountVectorizer(min_df=20)    # print(data['new_con'])    for feature in ['new_con']:        data[feature] = data[feature].astype(str)        # print(data[feature])        cv.fit(data[feature])        total_feature = sparse.hstack((total_feature, cv.transform(data[feature].astype(str))), 'csr', 'bool')    print('CountVectorizer_over!')    total_feature = sparse.hstack((sparse.csr_matrix(data[num_feature].astype('float32')), total_feature),                                  'csr').astype('float32')    print(total_feature)    return total_featuredef get_all_area(data):    df = pd.DataFrame()    data['小区总房屋面积'] = data['房屋面积'].sum()    print(data['小区总房屋面积'])    return data['小区总房屋面积']def countFeature(df=None):    data = copy.deepcopy(df)    columns = ["month_rent","department","district","position","direction"]    data = data[columns]    for i in columns[1:]:        tmp = data.groupby(i,as_index=False)["month_rent"].count().rename(columns = {"month_rent":"{0}_cnt".format(i)})        data = data.merge(tmp,how="left",on=i)    cnt_cols = list(filter(lambda x:"cnt" in x,data.columns))    return data[cnt_cols]def fun(df):    if df == 0:        r = 0    elif df == 1:        r = 0.3333    elif df == 2:        r = 0.6666    return rdef get_feature(data):    # data['卫的数量'] = (data['卫的数量'] * 10) ** 2    # data['卧室数量'] = (data['卧室数量'] * 10) ** 2    data['卧室_厅'] = data['卧室数量'] + data['厅的数量']    data['卧室_卫'] = data['卧室数量'] + data['卫的数量']    data['卫_厅'] = data['卫的数量'] + data['厅的数量']    data['卧室_厅_卫'] = data['卧室数量'] + data['厅的数量'] + data['卫的数量']    data['卧室占比'] = data['卧室数量'] / data['卧室_厅_卫']    data['卫的占比'] = data['卫的数量'] / data['卧室_厅_卫']    data['厅的占比'] = data['厅的数量'] / data['卧室_厅_卫']    data['average_面积'] = (data['房屋面积'] / 0.000165) / data['卧室_厅_卫']    data['面积/卧室'] = (data['房屋面积'] / 0.000165) / (data['卧室数量'] + 1)    data['面积/厅'] = (data['房屋面积'] / 0.000165) / (data['厅的数量'] + 1)    data['面积/卫'] = (data['房屋面积'] / 0.000165) / (data['卫的数量'] + 1)    # data['面积/卧室_卫'] = (data['房屋面积'] / 0.000165) / (data['卧室数量'] + data['卫的数量'])    # data['卫*厅'] = data['卫的数量'] * data['厅的数量']    data['卫*卧室'] = data['卫的数量'] * data['卧室数量']    data['总楼层'] = data['总楼层'] / 0.018182    data['小区房屋出租数量'] = data['小区房屋出租数量'] / 0.003906    data['房屋面积'] = (data['房屋面积'] / 0.000165)    data['总楼层房屋面积'] = data['房屋面积'] * data['总楼层']    # data['距离'] = data['距离'] / 0.000833    data['距离'] = data['距离'].fillna(-1)    data['距离'] = (data['距离'] / 0.000833).astype(int)    # data['距离'] = (data['距离'] / 50).astype(int)    # data['楼层1'] = data['楼层'].apply(lambda x: fun(x))    # data['所在楼层'] = data['楼层'] * data['总楼层']    data['楼层/总楼层'] = data['楼层'] / data['总楼层']    data['楼层-0.5总楼层'] = abs(data['楼层'] - data['总楼层'] * 0.5)    # data['卧室-卫'] = data['卧室数量'] - data['卫的数量']    # data['卧室-厅'] = data['卧室数量'] - data['厅的数量']    # data['小区房屋总面积'] = data.groupby(['小区房屋出租数量']).apply(lambda x: x['房屋面积'].sum())    # print(data['小区房屋总面积'].mean())    print(data.shape)    return data# 加特征后用这个train = pd.read_csv('../Data/train.csv')# train = train[:100]test = pd.read_csv('../Data/test.csv')# test = test[:100]train = get_feature(train)test = get_feature(test)# 删除没用的特征drop = []train.drop(drop, axis=1, inplace=True)test.drop(drop, axis=1, inplace=True)test['月租金'] = -1y_train = train.pop('月租金')res = pd.DataFrame()res['id'] = test.pop('id').astype(int)# train['总楼层'] = train['总楼层']*100# test['总楼层'] = test['总楼层']*100data = pd.concat([train, test], ignore_index=True)data1 = pd.DataFrame()def fill_na(df):    a = df['地铁线路'].max()    df['地铁线路'] = df['地铁线路'].fillna(a)    b = df['地铁站点'].max()    df['地铁站点'] = df['地铁站点'].fillna(b)    return df# data = data.groupby('小区名').apply(lambda x: fill_na(x))# data = data1data = data.fillna(-1)# print(data)data = get_cv(data)train = data[:train.shape[0]]print(train.shape)test = data[train.shape[0]:]print(test.shape)# train = data[data['月租金'] != -1].reset_index(drop=True)# y_train = train.pop('月租金')# test = data[data['月租金'] == -1].reset_index(drop=True)# test = test.drop(columns=['月租金'])X, y, X_test = train, y_train, testparams = {'boosting_type': 'gbdt',          'num_leaves': 88,          'max_depth': -1,          'objective': 'regression',          'learning_rate': 0.05,          # 0.03          'seed': 2018,          'num_threads': -1,          'max_bin': 425,          "metric": "rmse",          "bagging_fraction": 0.7,          # 0.7          'colsample_bytree': 0.9,          'subsample': 0.8,          # "lambda_l1": 0.1,          "lambda_l2": 0.2,          }# # 当前参数线下：1.24121# price mean:%d 8.033074515681161x_train = Xx_test = testx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.18, random_state=42)train_data = lgb.Dataset(x_train, label=y_train.values.flatten())val_data = lgb.Dataset(x_val, label=y_val.values.flatten())clf = lgb.train(params, train_set=train_data, num_boost_round=200000, valid_sets=[train_data, val_data],                  valid_names=['train', 'valid'], early_stopping_rounds=100, feval=None, verbose_eval=100)# importance = pd.DataFrame({#     'column': train.columns,#     'importance': clf.feature_importance(),# }).sort_values(by='importance')# pd.DataFrame(importance).to_csv('../data/feature_importance.csv', index=None)res['price'] = clf.predict(x_test)print('price mean:%d', res['price'].mean())# n = input('please input n:%d')# n = y.mean() / res['price'].mean()# res['price'] = res['price'] * int(n)now = datetime.datetime.now()now = now.strftime('%m-%d-%H-%M')res.to_csv("../Result/baseline1_xin.csv", index=None)end = time.time()print(datetime.datetime.now())print('all_time:', end-start)# model = lgb.LGBMRegressor(boosting_type='gbdt',#                           num_leaves=108,#                           max_depth=-1,#                           learning_rate=0.05,#                           n_estimators=10000,#                           max_bin=425,#                           subsample_for_bin=50000,#                           objective='regression',#                           min_split_gain=0,#                           min_child_weight=5,#                           min_child_samples=11,#                           subsample=0.8,#                           subsample_freq=1,#                           colsample_bytree=1,#                           reg_alpha=3,#                           reg_lambda=5,#                           seed=1000,#                           n_jobs=-1,#                           silent=True#                           )# X_loc_train = train.values# y_loc_train = y_train.values# X_loc_test = test.values## skf = list(StratifiedKFold(y_loc_train, n_folds=5, shuffle=True, random_state=1024))# basermse = []# rmse = 0# for i, (train_index, test_index) in enumerate(skf):#     print("Fold", i)#     lgb_model = model.fit(X_loc_train[train_index], y_loc_train[train_index],#                           eval_names=['train', 'valid'],#                           eval_metric='mse',#                           eval_set=[(X_loc_train[train_index], y_loc_train[train_index]),#                                     (X_loc_train[test_index], y_loc_train[test_index])], early_stopping_rounds=100,#                           verbose=50)#     basermse.append(np.sqrt(lgb_model.best_score_['valid']['regression_mse']))#     rmse += np.sqrt(lgb_model.best_score_['valid']['regression_mse'])#     test_pred = lgb_model.predict_proba(X_loc_test, num_iteration=lgb_model.best_iteration_)[:, 1]#     res['predicted_score'] = test_pred#     # print(res)#     print('test mean:', test_pred.mean())#     res['prob_%s' % str(i)] = test_pred# print('rmse:', basermse, rmse / 5)## # 加权平均# res['predicted_score'] = 0# for i in range(5):#     # print(res['prob_%s' % str(i)])#     res['predicted_score'] += res['prob_%s' % str(i)]# res['price'] = res['predicted_score'] / 5## # 提交结果# mean = res['price'].mean()# print('mean:', mean)# now = datetime.datetime.now()# now = now.strftime('%m-%d-%H-%M')# res[['id', 'price']].to_csv("../result/lgb_baseline_"+now+".csv", index=False)# model = lgb.LGBMRegressor(boosting_type='gbdt',#                           num_leaves=108,#                           max_depth=-1,#                           learning_rate=0.05,#                           n_estimators=10000,#                           max_bin=425,#                           subsample_for_bin=50000,#                           objective='regression',#                           min_split_gain=0,#                           min_child_weight=5,#                           min_child_samples=11,#                           subsample=0.8,#                           subsample_freq=1,#                           colsample_bytree=1,#                           reg_alpha=3,#                           reg_lambda=5,#                           seed=1000,#                           n_jobs=-1,#                           silent=True#                           )# x_train, x_val, y_train, y_val = train_test_split()# model.fit()